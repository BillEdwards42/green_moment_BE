# ==============================================================================
# SCRIPT: live_pipeline_final.py (v15.0 - Comprehensive Fix)
# PURPOSE:
#   - Complete rewrite to ensure correctness.
#   - All previous fixes integrated.
#   - Ensures all regions, including 'Other' and 'Unknown', are processed and saved.
#   - Corrected sanitize_name function for proper parsing.
#   - Removed problematic fillna(-99.0) for weather data.
#   - Ensured weather data is attempted for all regions.
# ==============================================================================
import pandas as pd
from pathlib import Path
import sys, re, json, requests, pytz, time
from datetime import datetime, timedelta
import numpy as np

# --- Configuration ---
BASE_DIR = Path(__file__).parent
FINAL_OUTPUT_DIR = BASE_DIR / "final_data"
FORECAST_CACHE_DIR = BASE_DIR / "forecast_cache"

PLANT_MAP_FILE = BASE_DIR / "plant_to_region_map.csv"
STATE_FILE = BASE_DIR / "last_run_units.json"
LOG_FILE = BASE_DIR / "fluctuation_log.txt"

TAIWAN_TZ = pytz.timezone('Asia/Taipei')
DATA_URL = "https://www.taipower.com.tw/d006/loadGraph/loadGraph/data/genary.json"

# --- Region-Specific Weather Logic Configuration ---
WEATHER_CONFIG = {
    'North': {
        'avg_towns': [('æ–°åŒ—å¸‚', 'æ—å£å€'), ('æ¡ƒåœ’å¸‚', 'è§€éŸ³å€'), ('è‹—æ —ç¸£', 'é€šéœ„é®'), ('è‡ºåŒ—å¸‚', 'ä¸­æ­£å€')],
        'code_town': ('è‡ºåŒ—å¸‚', 'ä¸­æ­£å€'),
        'forecast_files': ['æ–°åŒ—å¸‚_forecast.json', 'æ¡ƒåœ’å¸‚_forecast.json', 'è‹—æ —ç¸£_forecast.json', 'è‡ºåŒ—å¸‚_forecast.json']
    },
    'Central': {
        'avg_towns': [('è‡ºä¸­å¸‚', 'é¾äº•å€'), ('è‡ºä¸­å¸‚', 'è¥¿å±¯å€'), ('å½°åŒ–ç¸£', 'å½°åŒ–å¸‚')],
        'code_town': ('è‡ºä¸­å¸‚', 'è¥¿å±¯å€'),
        'forecast_files': ['è‡ºä¸­å¸‚_forecast.json', 'å½°åŒ–ç¸£_forecast.json']
    },
    'South': {
        'avg_towns': [('é«˜é›„å¸‚', 'æ°¸å®‰å€'), ('é«˜é›„å¸‚', 'å°æ¸¯å€'), ('è‡ºå—å¸‚', 'å®‰å—å€'), ('å±æ±ç¸£', 'æ†æ˜¥é®')],
        'code_town': ('é«˜é›„å¸‚', 'è‹“é›…å€'),
        'forecast_files': ['é«˜é›„å¸‚_forecast.json', 'è‡ºå—å¸‚_forecast.json', 'å±æ±ç¸£_forecast.json']
    },
    'East': {
        'avg_towns': [('èŠ±è“®ç¸£', 'èŠ±è“®å¸‚')],
        'code_town': ('èŠ±è“®ç¸£', 'èŠ±è“®å¸‚'),
        'forecast_files': ['èŠ±è“®ç¸£_forecast.json']
    },
    'Islands': {
        'avg_towns': [('æ¾æ¹–ç¸£', 'æ¹–è¥¿é„‰')],
        'code_town': ('æ¾æ¹–ç¸£', 'æ¹–è¥¿é„‰'),
        'forecast_files': ['æ¾æ¹–ç¸£_forecast.json']
    }
}

# --- Helper Functions ---
def sanitize_name(name):
    # Remove content within parentheses first
    name_without_parentheses = re.sub(r'\(.*\)', '', name)
    # Then sanitize invalid characters
    return re.sub(r'[\\/*?:"<>|]', '_', name_without_parentheses).strip()

def infer_region_from_name(unit_name):
    region_keywords = {
        'North': ['æ—å£', 'å¤§æ½­', 'æ–°æ¡ƒ', 'é€šéœ„', 'å”å’Œ', 'çŸ³é–€', 'ç¿¡ç¿ ', 'æ¡‚å±±', 'è§€éŸ³', 'é¾æ½­', 'åŒ—éƒ¨'],
        'Central': ['å°ä¸­', 'å¤§ç”²æºª', 'æ˜æ½­', 'å½°å·¥', 'ä¸­æ¸¯', 'ç«¹å—', 'è‹—æ —', 'é›²æ—', 'éº¥å¯®', 'ä¸­éƒ¨', 'å½°'],
        'South': ['èˆˆé”', 'å¤§æ—', 'å—éƒ¨', 'æ ¸ä¸‰', 'æ›¾æ–‡', 'å˜‰ç¾©', 'å°å—', 'é«˜é›„', 'æ°¸å®‰', 'å±æ±'],
        'East': ['å’Œå¹³', 'èŠ±è“®', 'è˜­é™½', 'å‘å—', 'ç«‹éœ§', 'æ±éƒ¨'], 
        'Islands': ['æ¾æ¹–', 'é‡‘é–€', 'é¦¬ç¥–', 'å¡”å±±', 'é›¢å³¶'],
        'Other': ['æ±½é›»å…±ç”Ÿ', 'å…¶ä»–å°é›»è‡ªæœ‰', 'å…¶ä»–è³¼é›»å¤ªé™½èƒ½', 'å…¶ä»–è³¼é›»é¢¨åŠ›', 'è³¼è²·åœ°ç†±', 'å°é›»è‡ªæœ‰åœ°ç†±', 'ç”Ÿè³ªèƒ½']
    }
    for region, keywords in region_keywords.items():
        if any(kw in str(unit_name) for kw in keywords): return region
    return None

def get_case_insensitive_key(d, key_variants, default=None):
    """Helper to get a value from a dict using a list of possible key casings."""
    for k in key_variants:
        if k in d:
            return d[k]
    return default

def get_forecast_value(forecast_json, town_name, element_name_chinese, target_time):
    try:
        records = get_case_insensitive_key(forecast_json, ['records', 'Records'])
        if not records: return None

        locations_list = get_case_insensitive_key(records, ['locations', 'Locations'])
        if not locations_list: return None

        locations_data = locations_list[0]
        location_list = get_case_insensitive_key(locations_data, ['location', 'Location']) or []

        town_data = next((loc for loc in location_list if get_case_insensitive_key(loc, ['locationName', 'LocationName']) == town_name), None)

        weather_element_list = None
        if town_data:
            weather_element_list = get_case_insensitive_key(town_data, ['weatherElement', 'WeatherElement'])
        if not weather_element_list:
            weather_element_list = get_case_insensitive_key(locations_data, ['weatherElement', 'WeatherElement'])

        if not weather_element_list:
            return None

        element_data = next((el for el in weather_element_list if get_case_insensitive_key(el, ['elementName', 'ElementName']) == element_name_chinese), None)
        if not element_data:
            return None

        time_blocks = get_case_insensitive_key(element_data, ["time", "Time"], default=[])
        if not time_blocks:
            return None

        # --- CORRECTED LOGIC ---
        # Find the time block where the target_time falls within [StartTime, EndTime)
        correct_block = None
        for time_block in time_blocks:
            start_time_str = get_case_insensitive_key(time_block, ['startTime', 'StartTime'])
            end_time_str = get_case_insensitive_key(time_block, ['endTime', 'EndTime'])
            
            if not start_time_str or not end_time_str:
                continue

            start_time = pd.to_datetime(start_time_str).tz_convert(TAIWAN_TZ)
            end_time = pd.to_datetime(end_time_str).tz_convert(TAIWAN_TZ)

            if start_time <= target_time < end_time:
                correct_block = time_block
                break  # Found the correct block, exit loop

        # If no block contains the target time (e.g., target_time is in the past), fall back to the first available block.
        if not correct_block and time_blocks:
            correct_block = time_blocks[0]
        # --- END CORRECTED LOGIC ---

        if correct_block:
            element_values = get_case_insensitive_key(correct_block, ['elementValue', 'ElementValue'], default=[])
            if not element_values: return None
            
            value_dict = element_values[0]
            value_str = None

            if element_name_chinese == 'å¤©æ°£ç¾è±¡':
                value_str = get_case_insensitive_key(value_dict, ['WeatherCode', 'weathercode'])
            else:
                if value_dict and isinstance(value_dict, dict):
                    value_str = next(iter(value_dict.values()), None)

            if value_str is not None and str(value_str).strip() and str(value_str) != '-':
                try:
                    return float(value_str)
                except ValueError:
                    return float(re.findall(r'\d+', value_str)[0])

    except (KeyError, IndexError, TypeError, ValueError):
        return None
    
    return None

def get_regional_weather_features(region, effective_time):
    # No longer skipping 'Other' or 'Unknown' regions here
    if region not in WEATHER_CONFIG: 
        return {} # Return empty dict if region not in WEATHER_CONFIG (e.g., 'Other', 'Unknown')
    
    config = WEATHER_CONFIG[region]
    features = {}
    forecasts = {}
    for f_name in config['forecast_files']:
        try:
            file_path = FORECAST_CACHE_DIR / f_name
            with open(file_path, 'r', encoding='utf-8') as f:
                county_name = f_name.replace('_forecast.json', '')
                forecasts[county_name] = json.load(f)
        except FileNotFoundError:
            print(f"  -> WARNING: Forecast for {f_name} not found in '{FORECAST_CACHE_DIR}'. Skipping {region}.")
            return {}

    for suffix, time_offset in [('_now', 0), ('_future_12h', 12)]:
        target_time = effective_time + timedelta(hours=time_offset)
        
        temps, winds = [], []
        for county, town in config['avg_towns']:
            if county in forecasts:
                temp = get_forecast_value(forecasts[county], town, 'å¹³å‡æº«åº¦', target_time)
                wind = get_forecast_value(forecasts[county], town, 'é¢¨é€Ÿ', target_time)
                if temp is not None: temps.append(temp)
                if wind is not None: winds.append(wind)
        
        features[f'TEMP{suffix}'] = round(np.mean(temps), 2) if temps else np.nan # Use np.nan instead of -99.0
        features[f'WIND{suffix}'] = round(np.mean(winds), 2) if winds else np.nan # Use np.nan instead of -99.0

        code_county, code_town = config['code_town']
        if code_county in forecasts:
            w_code = get_forecast_value(forecasts[code_county], code_town, 'å¤©æ°£ç¾è±¡', target_time)
            features[f'W_CODE{suffix}'] = int(w_code) if w_code is not None else np.nan # Use np.nan instead of -99
        else:
            features[f'W_CODE{suffix}'] = np.nan # Use np.nan instead of -99
            
    return features

def fetch_and_save_demand_data(effective_data_time, formatted_datetime):
    """Fetches and saves the current electricity demand data."""
    print("   -> Fetching current electricity demand...")
    DEMAND_URL = "https://www.taipower.com.tw/d006/loadGraph/loadGraph/data/loadpara.json"
    
    try:
        cache_bust = int(time.time())
        demand_url_with_bust = f"{DEMAND_URL}?_={cache_bust}"
        response = requests.get(demand_url_with_bust, timeout=20)
        response.raise_for_status()
        demand_data = response.json()
        
        # The key is 'curr_load' and it's inside the first element of the 'records' list
        if not demand_data.get('records') or not isinstance(demand_data['records'], list) or not demand_data['records']:
            print("  -> WARNING: 'records' array not found or is empty in demand data. Skipping.")
            print(f"  -> DEBUG: Full demand data response: {demand_data}")
            return

        current_load_str = demand_data['records'][0].get('curr_load')
        
        if not current_load_str:
            print("  -> WARNING: 'curr_load' key not found in demand data records. Skipping.")
            print(f"  -> DEBUG: Full demand data response: {demand_data}")
            return
            
        current_load_mw = float(current_load_str.replace(',', ''))
        
        demand_df = pd.DataFrame([{
            'DATETIME': formatted_datetime,
            'DEMAND_MW': current_load_mw
        }])
        
        output_path = FINAL_OUTPUT_DIR / "electricity_demand.csv"
        
        # Append new data directly
        demand_df.to_csv(output_path, mode='a', header=not output_path.exists(), index=False, encoding='utf-8-sig')
        print(f"   -> Saved current demand ({current_load_mw} MW) to {output_path.name}.")

    except requests.exceptions.RequestException as e:
        print(f"  -> WARNING: Failed to fetch Taipower demand data: {e}")
    except (ValueError, TypeError, KeyError) as e:
        print(f"  -> WARNING: Failed to parse or process demand data: {e}")

def run_pipeline():
    run_time = datetime.now(TAIWAN_TZ)
    effective_minute = (run_time.minute // 10) * 10
    effective_data_time = run_time.replace(minute=effective_minute, second=0, microsecond=0)
    formatted_datetime = effective_data_time.strftime('%Y-%m-%d %H:%M:%S')
    
    print(f"[{run_time.strftime('%Y-%m-%d %H:%M:%S')}] --- Running Pipeline for {formatted_datetime} ---")

    try:
        # Add a cache-busting parameter to the URL
        cache_bust = int(time.time())
        data_url_with_bust = f"{DATA_URL}?_={cache_bust}"
        response = requests.get(data_url_with_bust, timeout=20)
        response.raise_for_status()
        live_data = response.json().get('aaData', [])
    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ FAILED to fetch Taipower data: {e}")
        return

    # --- Fetch and Save Demand Data ---
    fetch_and_save_demand_data(effective_data_time, formatted_datetime)
    # --- End Demand Data ---

    records = []
    fuel_map = {
        'å¤ªé™½èƒ½': 'å¤ªé™½èƒ½(Solar)',
        'é¢¨åŠ›': 'é¢¨åŠ›(Wind)',
        'ç‡ƒç…¤': 'ç‡ƒç…¤(Coal)',
        'ç‡ƒæ°£': 'ç‡ƒæ°£(LNG)',
        'æ°´åŠ›': 'æ°´åŠ›(Hydro)',
        'æ ¸èƒ½': 'æ ¸èƒ½(Nuclear)',
        'æ±½é›»å…±ç”Ÿ': 'æ±½é›»å…±ç”Ÿ(Co-Gen)',
        'æ°‘ç‡Ÿé›»å» -ç‡ƒç…¤': 'æ°‘ç‡Ÿé›»å» -ç‡ƒç…¤(IPP-Coal)',
        'æ°‘ç‡Ÿé›»å» -ç‡ƒæ°£': 'æ°‘ç‡Ÿé›»å» -ç‡ƒæ°£(IPP-LNG)',
        'ç‡ƒæ²¹': 'ç‡ƒæ²¹(Oil)',
        'è¼•æ²¹': 'è¼•æ²¹(Diesel)',
        'å…¶å®ƒå†ç”Ÿèƒ½æº': 'å…¶å®ƒå†ç”Ÿèƒ½æº(Other Renewable Energy)',
        'å„²èƒ½': 'å„²èƒ½(Energy Storage System)'
    }
    for row in live_data:
        if len(row) < 5 or 'å°è¨ˆ' in row[2]: continue
        unit_name, net_p_str = row[2].strip(), str(row[4]).replace(',', '')
        match = re.search(r'<b>(.*?)</b>', row[0])
        if not match or not unit_name or 'Load' in match.group(1): continue
        fuel_type = fuel_map.get(match.group(1), match.group(1))
        net_p = float(net_p_str) if re.match(r'^-?\d+(\.\d+)?$', net_p_str) else None
        if net_p is not None:
            records.append({'DATETIME': formatted_datetime, 'FUEL_TYPE': fuel_type, 'UNIT_NAME': unit_name, 'NET_P': net_p})

    if not records:
        print("No valid generator records found. Exiting.")
        return

    print(f"   -> Fetched data for {len(records)} active power plant units.")
    new_data_df = pd.DataFrame(records)

    try:
        with open(STATE_FILE, 'r') as f:
            previous_units = set(json.load(f))
    except (FileNotFoundError, json.JSONDecodeError):
        previous_units = set()
    
    current_units = set(new_data_df['UNIT_NAME'])

    print(f"DEBUG: Previous units: {previous_units}")
    print(f"DEBUG: Current units: {current_units}")

    newly_added, missing = current_units - previous_units, previous_units - current_units
    
    log_status_symbol = 'âœ…' if not newly_added and not missing else 'âŒ'
    log_message = f"--- Fluctuation Report @ {formatted_datetime} ({len(current_units)} plants) {log_status_symbol} ---\n"
    if newly_added: log_message += f"  [ADDED] {', '.join(sorted(list(newly_added)))} \n"
    if missing: log_message += f"  [MISSING] {', '.join(sorted(list(missing)))} \n"
    
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(log_message)
    print(f"   -> Fluctuation log updated. Added: {len(newly_added)}, Missing: {len(missing)}.")
    with open(STATE_FILE, 'w') as f: json.dump(list(current_units), f)

    print("   -> Assigning regions using 3-layer logic...")
    try:
        plant_map_df = pd.read_csv(PLANT_MAP_FILE)
        df_merged = pd.merge(new_data_df, plant_map_df, on='UNIT_NAME', how='left')
    except FileNotFoundError:
        print(f"   -> INFO: '{PLANT_MAP_FILE}' not found. Skipping CSV mapping.")
        df_merged = new_data_df
        df_merged['REGION'] = None
    unmapped_mask = df_merged['REGION'].isna()
    df_merged.loc[unmapped_mask, 'REGION'] = df_merged.loc[unmapped_mask, 'UNIT_NAME'].apply(infer_region_from_name)
    df_merged['REGION'].fillna('Unknown', inplace=True)
    print(f"DEBUG: Unique regions after assignment: {df_merged['REGION'].unique()}")
    df_merged['REGION'] = df_merged['REGION'].str.strip()

    # --- Unknown Plants Logging ---
    unknown_plants_df = df_merged[df_merged['REGION'] == 'Unknown']
    if not unknown_plants_df.empty:
        unknown_plant_names = sorted(unknown_plants_df['UNIT_NAME'].tolist())
        unknown_log_message = f"[{formatted_datetime}] âŒ Unknown Plants Detected:\n"
        unknown_log_message += f"  {', '.join(unknown_plant_names)}\n"
        print(f"   -> Unknown plants logged: {len(unknown_plant_names)}.")
    else:
        unknown_log_message = f"[{formatted_datetime}] âœ… No Unknown Plants Detected.\n"
        print("   -> No unknown plants detected.")

    UNKNOWN_PLANTS_LOG_FILE = BASE_DIR / "unknown_plants_log.txt"
    with open(UNKNOWN_PLANTS_LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(unknown_log_message)
    # --- End Unknown Plants Logging ---

    # --- Unit Details Logging ---
    UNIT_DETAILS_LOG_FILE = BASE_DIR / "unit_details_log.csv"
    unit_details_df = df_merged[['DATETIME', 'UNIT_NAME', 'REGION', 'FUEL_TYPE']].copy()
    unit_details_df.to_csv(UNIT_DETAILS_LOG_FILE, mode='a', header=not UNIT_DETAILS_LOG_FILE.exists(), index=False, encoding='utf-8-sig')
    print(f"   -> Appended {len(unit_details_df)} unit details to {UNIT_DETAILS_LOG_FILE.name}.")
    # --- End Unit Details Logging ---
    
    print("   -> Enriching with weather data from local cache...")
    regional_weather_cache = {}
    for region in df_merged['REGION'].unique():
        # Removed the condition that skips fetching weather data for 'Other' or 'Unknown' regions
        regional_weather_cache[region] = get_regional_weather_features(region, effective_data_time)
    weather_df = df_merged['REGION'].map(regional_weather_cache).apply(pd.Series)
    df_enriched = pd.concat([df_merged.drop(columns='UNIT_NAME'), weather_df], axis=1)

    agg_funcs = {
        'NET_P': 'sum', 'TEMP_now': 'first', 'WIND_now': 'first', 'W_CODE_now': 'first',
        'TEMP_future_12h': 'first', 'WIND_future_12h': 'first', 'W_CODE_future_12h': 'first'
    }
    
    aggregated_data = df_enriched.groupby(['DATETIME', 'REGION', 'FUEL_TYPE']).agg(agg_funcs).reset_index()
    print("   -> Aggregated new data with weather into final format.")

    FINAL_OUTPUT_DIR.mkdir(exist_ok=True)
    for (region, fuel_type), group in aggregated_data.groupby(['REGION', 'FUEL_TYPE']):
        
        fuel_folder_name = sanitize_name(fuel_type)
        region_name_safe = sanitize_name(str(region))
        print(f"DEBUG: Sanitized Fuel Folder Name: {fuel_folder_name}")
        print(f"DEBUG: Sanitized Region Name: {region_name_safe}")
        
        # Create folder based on region
        region_folder_path = FINAL_OUTPUT_DIR / region_name_safe
        region_folder_path.mkdir(exist_ok=True)
        
        # File name based on region and fuel type
        output_path = region_folder_path / f"{region_name_safe}_{fuel_folder_name}.csv"

        # Read existing data, filter out current timestamp, and append new data
        if output_path.exists():
            existing_df = pd.read_csv(output_path)
            existing_df['DATETIME'] = pd.to_datetime(existing_df['DATETIME'])
            # Filter out rows with the current formatted_datetime to avoid duplicates
            existing_df = existing_df[pd.to_datetime(existing_df['DATETIME']) != pd.to_datetime(formatted_datetime)]
            combined_df = pd.concat([existing_df, group], ignore_index=True)
        else:
            combined_df = group
        
        combined_df.to_csv(output_path, mode='w', header=True, index=False, encoding='utf-8-sig')
    
    print("   -> Appended enriched data to final segmented files.")
    print(f"[{datetime.now(TAIWAN_TZ).strftime('%Y-%m-%d %H:%M:%S')}] --- Pipeline Run Successful ---")

if __name__ == "__main__":
    run_pipeline()