# ==============================================================================
# SCRIPT: fetch_realtime_weather.py
# PURPOSE:
#   - Fetches real-time weather observations for power generation-relevant stations.
#   - Designed to be run once by an external scheduler (e.g., cronjob).
#   - Calculates and saves the average Sunshine Duration, Air Temperature, and
#     Wind Speed for 5 predefined regions into separate JSON files.
#   - Logs the individual data for each station from every fetch into a
#     persistent CSV log, flagging any null/missing values.
# ==============================================================================
import os
import requests
import json
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime
import csv
import time

# --- Configuration ---
# Load environment variables from a .env file
# å¾ .env æ–‡ä»¶åŠ è¼‰ç’°å¢ƒè®Šæ•¸
load_dotenv()
CWA_API_KEY = os.getenv("CWA_API_KEY")
if not CWA_API_KEY:
    # Fallback to the provided key if not found in environment
    # å¦‚æœç’°å¢ƒä¸­æ‰¾ä¸åˆ°ï¼Œå‰‡ä½¿ç”¨é è¨­çš„ API é‡‘é‘°
    CWA_API_KEY = "CWA-498C56D4-B151-4539-992D-B2CB97042454"

# API endpoint for real-time observations from manned stations
# å³æ™‚è§€æ¸¬è³‡æ–™ API ç«¯é» (æœ‰äººé§å®ˆç«™)
BASE_API_URL = "https://opendata.cwa.gov.tw/api/v1/rest/datastore/O-A0003-001"

# Define the base directory for dynamic path creation relative to the script's location
# å®šç¾©ç›¸å°æ–¼è…³æœ¬ä½ç½®çš„åŸºç¤ç›®éŒ„ä»¥å‹•æ…‹å‰µå»ºè·¯å¾‘
BASE_DIR = Path(__file__).parent
# Directory to store the averaged regional data
# å­˜å„²å€åŸŸå¹³å‡æ•¸æ“šçš„ç›®éŒ„
WEATHER_DATA_DIR = BASE_DIR / "weather_data"
# Log file for individual station data from each fetch
# æ¯æ¬¡æ“·å–çš„å„æ¸¬ç«™ç¨ç«‹æ•¸æ“šæ—¥èªŒæ–‡ä»¶
WEATHER_LOG_FILE = BASE_DIR / "10min_weather_log.csv"

# Stations relevant to power generation, grouped by region
# èˆ‡ç™¼é›»ç›¸é—œçš„æ°£è±¡ç«™ï¼ŒæŒ‰å€åŸŸåˆ†çµ„
STATIONS_BY_REGION = {
    "north": ["è‡ºåŒ—", "æ–°åŒ—", "åŸºéš†", "æ–°ç«¹", "æ–°å±‹", "ééƒ¨"],
    "central": ["è‡ºä¸­", "å¾Œé¾", "å¤å‘", "ç”°ä¸­", "æ—¥æœˆæ½­", "é˜¿é‡Œå±±", "ç‰å±±"],
    "south": ["å˜‰ç¾©", "è‡ºå—", "æ°¸åº·", "é«˜é›„", "æ†æ˜¥"],
    "east": ["å®œè˜­", "èŠ±è“®", "æˆåŠŸ", "è‡ºæ±", "å¤§æ­¦"],
    "island": ["æ¾æ¹–", "é‡‘é–€", "é¦¬ç¥–", "è˜­å¶¼", "æ±å‰å³¶"]
}

# Fields to extract and average
# è¦æå–å’Œå¹³å‡çš„æ¬„ä½
TARGET_FIELDS = ["SunshineDuration", "AirTemperature", "WindSpeed"]

# --- Helper Functions ---

def safe_float_convert(value):
    """
    Safely converts a value to float, handling CWA's null identifiers.
    å®‰å…¨åœ°å°‡å€¼è½‰æ›ç‚ºæµ®é»æ•¸ï¼Œä¸¦è™•ç†ä¸­å¤®æ°£è±¡ç½²çš„ç„¡æ•ˆå€¼æ¨™è­˜ã€‚
    Returns the float value or None if conversion is not possible.
    è¿”å›æµ®é»æ•¸å€¼ï¼Œå¦‚æœç„¡æ³•è½‰æ›å‰‡è¿”å› Noneã€‚
    """
    try:
        # CWA uses specific negative numbers to indicate missing data
        # ä¸­å¤®æ°£è±¡ç½²ä½¿ç”¨ç‰¹å®šçš„è² æ•¸ä¾†è¡¨ç¤ºç¼ºå¤±æ•¸æ“š
        if float(value) < -90:
            return None
        return float(value)
    except (ValueError, TypeError):
        return None

def fetch_weather_data():
    """
    Fetches the latest weather observation data from the CWA API.
    å¾ä¸­å¤®æ°£è±¡ç½² API æ“·å–æœ€æ–°çš„æ°£è±¡è§€æ¸¬è³‡æ–™ã€‚
    """
    params = {"Authorization": CWA_API_KEY}
    print(f"ğŸ“¡ [{datetime.now().strftime('%H:%M:%S')}] Fetching real-time weather data from {BASE_API_URL}...")
    try:
        response = requests.get(BASE_API_URL, params=params, timeout=30)
        response.raise_for_status()
        print(f"âœ… SUCCESS: API response received. Status: {response.status_code}")
        return response.json()
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTP ERROR: {e}\n   -> Response: {response.text}")
    except Exception as e:
        print(f"âŒ UNEXPECTED ERROR during fetch: {e}")
    return None

def get_last_obs_time(filepath):
    """
    Reads the last ObsTime from a regional CSV file to avoid duplicates.
    å¾å€åŸŸ CSV æª”æ¡ˆä¸­è®€å–æœ€å¾Œçš„è§€æ¸¬æ™‚é–“ä»¥é¿å…é‡è¤‡ã€‚
    """
    if not filepath.exists():
        return None
    try:
        with open(filepath, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            last_row = None
            for row in reader:
                last_row = row
            if last_row:
                return last_row.get("ObsTime")
    except (IOError, KeyError, StopIteration) as e:
        # StopIteration can happen on empty files with some csv reader versions
        print(f"âš ï¸  Could not read last timestamp from {filepath} (file might be empty or corrupt). Error: {e}")
    return None

def process_and_log_data(api_data):
    """
    Processes the fetched data to calculate regional averages,
    append them to regional CSV files, and log individual station data.
    è™•ç†æ“·å–çš„è³‡æ–™ï¼Œè¨ˆç®—å€åŸŸå¹³å‡å€¼ï¼Œå°‡å…¶é™„åŠ åˆ°å€åŸŸ CSV æª”æ¡ˆï¼Œä¸¦è¨˜éŒ„å„æ¸¬ç«™çš„æ•¸æ“šã€‚
    """
    # CORRECTED PATH: The station data is under the 'records' key for this API endpoint.
    # ä¿®æ­£è·¯å¾‘ï¼šæ­¤ API ç«¯é»çš„æ¸¬ç«™è³‡æ–™ä½æ–¼ 'records' éµä¸‹ã€‚
    try:
        all_stations_data = api_data['records']['Station']
        if not all_stations_data:
            print("âš ï¸ WARNING: API response contains no station data.")
            return
    except KeyError:
        print("âŒ ERROR: Could not find a valid 'records' or 'Station' structure in the API data. Aborting.")
        return

    # --- 1. Process and Log Individual Station Data ---
    log_rows = []
    processed_stations = {}
    valid_timestamp = None

    for station in all_stations_data:
        station_name = station.get("StationName")
        if not any(station_name in sl for sl in STATIONS_BY_REGION.values()):
            continue # Skip stations not in our list
        
        if not valid_timestamp:
            valid_timestamp = station.get("ObsTime", {}).get("DateTime")

        elements = station.get("WeatherElement", {})
        
        temp = safe_float_convert(elements.get("AirTemperature"))
        wind = safe_float_convert(elements.get("WindSpeed"))
        sunshine = safe_float_convert(elements.get("SunshineDuration"))
        
        has_null = any(v is None for v in [temp, wind, sunshine])
        
        processed_stations[station_name] = {
            "AirTemperature": temp,
            "WindSpeed": wind,
            "SunshineDuration": sunshine
        }

        log_rows.append({
            "Timestamp": station.get("ObsTime", {}).get("DateTime"),
            "StationName": station_name,
            "AirTemperature": temp if temp is not None else 'NULL',
            "WindSpeed": wind if wind is not None else 'NULL',
            "SunshineDuration": sunshine if sunshine is not None else 'NULL',
            "HasNullValue": has_null
        })

    if not log_rows:
        print("âš ï¸ No relevant stations found in the fetched data. No logs or data files will be updated.")
        return
    
    if not valid_timestamp:
        print("âš ï¸ Could not determine a valid observation timestamp from the data. Aborting regional processing.")
        return

    # Append to the 10-minute log file
    log_file_exists = WEATHER_LOG_FILE.exists()
    with open(WEATHER_LOG_FILE, 'a', newline='', encoding='utf-8') as csvfile:
        fieldnames = ["Timestamp", "StationName", "AirTemperature", "WindSpeed", "SunshineDuration", "HasNullValue"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not log_file_exists:
            writer.writeheader()
        writer.writerows(log_rows)
    print(f"ğŸ“ {len(log_rows)} station records for timestamp {valid_timestamp} appended to {WEATHER_LOG_FILE}")

    # --- 2. Calculate and Append Regional Averages to CSV ---
    for region, station_list in STATIONS_BY_REGION.items():
        output_path = WEATHER_DATA_DIR / f"{region}.csv"
        
        # Check if data for this timestamp already exists
        last_timestamp = get_last_obs_time(output_path)
        if last_timestamp and last_timestamp == valid_timestamp:
            print(f"âœ… Data for {region} is already up-to-date (Timestamp: {valid_timestamp}). Skipping append.")
            continue

        regional_averages = {}
        for field in TARGET_FIELDS:
            values = [
                processed_stations[s_name][field] 
                for s_name in station_list 
                if s_name in processed_stations and processed_stations[s_name][field] is not None
            ]
            
            if values:
                average = sum(values) / len(values)
                regional_averages[field] = round(average, 2)
            else:
                regional_averages[field] = None

        csv_row = {
            "ObsTime": valid_timestamp,
            "SunshineDuration": regional_averages.get("SunshineDuration"),
            "AirTemperature": regional_averages.get("AirTemperature"),
            "WindSpeed": regional_averages.get("WindSpeed")
        }

        file_exists = output_path.exists()
        with open(output_path, 'a', newline='', encoding='utf-8') as f:
            fieldnames = ["ObsTime", "SunshineDuration", "AirTemperature", "WindSpeed"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            if not file_exists:
                writer.writeheader()
            writer.writerow(csv_row)
            
        print(f"ğŸ’¾ New regional average for {region} appended to {output_path}")


# --- Main Execution Block ---
if __name__ == "__main__":
    # Create the output directory if it doesn't exist
    # å¦‚æœè¼¸å‡ºç›®éŒ„ä¸å­˜åœ¨ï¼Œå‰‡å‰µå»ºå®ƒ
    WEATHER_DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    print("--- Starting Real-time Weather Fetch ---")
    print(f"Source Directory: {BASE_DIR}")
    print(f"Data will be saved in: {WEATHER_DATA_DIR}")
    print(f"Individual logs will be saved to: {WEATHER_LOG_FILE}")
    
    # Fetch and process the data once.
    # æ“·å–ä¸¦è™•ç†ä¸€æ¬¡è³‡æ–™ã€‚
    api_response_data = fetch_weather_data()
    if api_response_data:
        process_and_log_data(api_response_data)
    
    print("--- Fetch and Log Complete ---")